{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Summary\n",
    "\n",
    "Lesson 4, 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Structured Languages vs Unstructured Languages\n",
    "    - SQL같은 것은 structure이 정확하게 짜여있음. ambiguity가 없음. \n",
    "    - Natural Language는 grammar이라는 structure이 있지만, ambiguity가 있는 unstructured data임. \n",
    "        - Parsing을 통해 컴퓨터가 알아듣게 만들 수 있음. \n",
    "        - It 같은 대명사는 문장 구조가 같아도 Sementic에 따라 의미가 달라지므로 컴퓨터가 알아듣기 어려운 부분이 있다. \n",
    "- NLP Pipeline\n",
    "    1. Text Processing\n",
    "        - preprocessing, normalizing, numeric input format \n",
    "        - html/pdf parsing, tts, ocr 등. \n",
    "        - stopwords, capitalization, 등도 포함. \n",
    "    2. Feature Extraction\n",
    "        - feature representation\n",
    "        - 알파벳이 아닌, 단어의 sequence가 의미있는 것이다. 이에 맞게 respresentation 해줘야 함. \n",
    "        - word level, sentence level, document level 등 어떤 레벨로 할 것인가에 따라 적절한 representation 방법이 달라질 수 있다. \n",
    "            - document: bag of words, tf-idf\n",
    "            - word: WordNet (graph) 등\n",
    "    3. Modeling\n",
    "        - numerical representation 만들어놓으면 모델은 여러 가지 선택 가능하다. \n",
    "        - unseen data에 대해 예측력을 가지는 것이 모델의 목표\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Text Processing\n",
    "    - plain text (long string)\n",
    "    - tabular data\n",
    "        - date, author, publisher, title, body 등 \n",
    "    - 많은 API의 형식인 JSON, XML 등 파싱도 필요 \n",
    "- Normalization\n",
    "    - 단순한 전처리\n",
    "        - lower case\n",
    "        - punctuation removal \n",
    "            - regex substitution 쓸 수 있다. 다른 방법도 되고. \n",
    "- Tokenization\n",
    "    - 간단히는 spacing을 이용한 wordpiece도 되고 \n",
    "    - 진짜 token으로 나눌 수도 있다. \n",
    "        - `nltk`: `word_tokenize`, `sent_tokenize`\n",
    "            - twitter 용으로 twitter handle, hashtag 등을 인식하는 tokenizer도 있다. \n",
    "- Cleaning\n",
    "    - `BeautifulSoup`로 html을 파싱하자. \n",
    "    - stopwords는 `nltk.corpus` 에서 언어별로 가져와서 없애자. \n",
    "- POS (Part of Speech) Tagging\n",
    "    - `nltk`의 `pos_tag()` 사용. \n",
    "        - 다분히 언어학적인 approach. 요즘 트랜드는 아닌듯. \n",
    "- NER (Named Entity Recognition)\n",
    "    - `nltk`의 `ne_chunk` 사용 가능. \n",
    "    - 하지만 이제 deep learning 라이브러리가 많으니 그걸 쓰자. \n",
    "- Stemming & Lemmatization\n",
    "    - Stemming, 어근화. \n",
    "        - 언어별로, 다양한 stemmer 선택 가능. \n",
    "    - lemmatization\n",
    "        - 언어의 기본형으로 바꿈. am, are --> be \n",
    "        - `nltk.stem.wordnet`의 `WordNetLemmatizer`같은거 사용. \n",
    "        - stemming과의 차이: stem과 달리 lemma는 meaningful word이다. transform 거치고나서도 쓸 수 있는 단어가 남음.  \n",
    "- 전체적인 과정: <솔직히 아주 좋은 과정같진 않음.>\n",
    "    1. Text processing으로 기본 전처리를 하고 \n",
    "    2. Normalization으로 tokenization에 방해되는 punctuation 등 날려주고\n",
    "    3. tokenize하고 \n",
    "    4. remove stopwords 등으로 한 번 필터하고\n",
    "    5. stemming/lemmatization으로 기본형으로 바꾸기 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 단순 Bag-of-Words로 vector encoding\n",
    "    - corpus를 이용해 one-hot으로 encoding \n",
    "    - 이 vector 끼리의 cosine similarity를 비교해서 유사도를 구할 수 있음. \n",
    "        - -1 ~ 1\n",
    "- TF-IDF (이것도 BOW류, count 기반)\n",
    "    - TF: number of times the term appears in the document / total number of terms in the document\n",
    "        - 한 문서에서 이 단어가 얼마나 자주 나오는지? \n",
    "        - 각 word에 대해, TF는 document 단위로 값이 나옴. (document 수만큼)\n",
    "    - IDF: log(number of the documents in the corpus / number of documents in the corpus contain the term)\n",
    "        - 각 word에 대해, IDF는 corpus 단위로 값이 나옴. 값이 1개. \n",
    "    - TF-IDF: \n",
    "        - 각 word에 대해, TF-IDF도 document 단위로 값이 나옴. \n",
    "        - document를 1줄의 vector로 표현할 때, 배열의 각 자리가 word를 뜻한다고 하고 값을 그 문서의 TF-IDF로 채워넣으면 document를 vector로 representation 할 수 있음. \n",
    "- Word2Vec\n",
    "    - word를 vector로 embedding \n",
    "        - Distributional Hypothesis에 근거. 유사할수록 같이 자주 나온다. \n",
    "        - vector size는 one-hot encoding하는 dimension보다 적게 해주는 것이 중요함. 그렇지 않으면 의미 없음. \n",
    "    - 이 역시 sequence의 window 내의 단어들의 순서는 고려하지 않는다는 점에서 count 기반. \n",
    "        - CBOW (continuous bag of words)\n",
    "            - 5개 단어 중 가운데를 가리고, 가운데 단어를 맞춘다. \n",
    "        - Skipgram\n",
    "            - 5개의 단어 중 가운데만 빼고 나머지를 다 가리고, 나머지를 맞춘다. \n",
    "    - word2vec의 특성\n",
    "        - vector size가 corpus내의 vocabulary에 따라 좌우되지 않음. (TF-IDF는 vocabulary가 배열 길이 결정함.)\n",
    "        - vector size가 일정하니 행렬로 전체 데이터 표현 가능해 ML/DL에 input으로 넣기 좋음. \n",
    "        - 한 번 학습시키고 vector representation 나오면 look-up table에 넣어두면 됨. \n",
    "- GloVe (Global Vectors for Word Representation)\n",
    "    - Co-occurance Matrix를 이용: 각 context window에서 몇 번이나 word가 발생하는지 matrix를 만듦\n",
    "    - 비슷한 context에서 같이 나오는 단어들은 비슷해지도록 학습됨. \n",
    "- Embedding과 딥러닝\n",
    "    - word2vec, GloVe로 만든 representation을 딥러닝 input으로 넣어주는 것도 가능하다. \n",
    "- t-SNE (t-Distributed Stochastic Neighbor Embedding)\n",
    "    - PCA처럼 고차원 벡터를 저차원으로 바꾸는 것. (but vector들의 relative distance를 유지하며.)\n",
    "    - NLP, image 등 다 쓰일 수 있음. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SEC 문서로 trading signal 만들기 \n",
    "    - 10-K (연간 사업보고서), \n",
    "        1. Business Overview\n",
    "            - risk, possible legal proceeding에 대한 정보\n",
    "        2. Markets/Finance\n",
    "            - market condition, projection 등\n",
    "        3. Governance\n",
    "            - 이사회 정보 (변동, 연봉 등)\n",
    "        4. Full Financials\n",
    "            - 재무제표\n",
    "    - 10-Q (분기 사업보고서, 4분기 빼고)\n",
    "    - 8-K (수시보고서)\n",
    "    - EDGAR(Electronic Data Gathering And Retrieval)\n",
    "- 10-K 맛보기\n",
    "    - sec.gov에 들어가, FILINGS > company filing에서 찾는다. \n",
    "    - ticker로 찾을 수도 있고, 검색도 가능한데, Apple 이 들어가는 다른 기업이 있을 수 있어서 CIK라는 key로 구분해야 한다. \n",
    "    - 10-K form을 정돈된 html로 볼 수도 있지만, 90년대 등 예전 filing은 plain text만 제공되기 때문에 plain text를 기준으로 parsing해야 함. \n",
    "- 10-K에서 sentiment 뽑아내기 \n",
    "    - 부정적 단어/긍정적 word/phrase 언급을 통해서도 아주 간단히 signal 찾을 수 있음. (word search 기반)\n",
    "    - `re` regex 정규식 사용법 익혀두자. \n",
    "        - regex로 전화번호, 금액 형식 등을 pattern search 할 수 있음.\n",
    "        - `re.finditer()` 사용해 모든 match 찾을 수 있음. \n",
    "        - backslash로 시작하는 다양한 문법 익혀두면 좋음. \n",
    "    - `BeautifulSoup` 사용법도 익혀두자\n",
    "        - Tree 구조인 html/xml 파싱\n",
    "        - parsing tree를 통해 원하는 element를 찾을 수 있음. \n",
    "        - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
